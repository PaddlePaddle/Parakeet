data:
  batch_size: 8
  train_clip_seconds: 0.5
  sample_rate: 22050
  hop_length: 256
  win_length: 1024
  n_fft: 2048
  
  n_mels: 80
  valid_size: 16


conditioner:
  upsampling_factors: [16, 16]

teacher:
  n_loop: 10
  n_layer: 3
  filter_size: 2
  residual_channels: 128
  loss_type: "mog"
  output_dim: 3
  log_scale_min: -9

student:
  n_loops: [10, 10, 10, 10, 10, 10]
  n_layers: [1, 1, 1, 1, 1, 1]
  filter_size: 3
  residual_channels: 64
  log_scale_min: -7

stft:
  n_fft: 2048
  win_length: 1024
  hop_length: 256

loss:
  lmd: 4

train:
  learning_rate: 0.0005
  anneal_rate:  0.5
  anneal_interval: 200000
  gradient_max_norm: 100.0

  checkpoint_interval: 1000
  eval_interval: 1000
  
  max_iterations: 2000000



